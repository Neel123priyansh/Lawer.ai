{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13069303,"sourceType":"datasetVersion","datasetId":8276923}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain-huggingface langchain-pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:54:08.482083Z","iopub.execute_input":"2025-09-15T17:54:08.482705Z","iopub.status.idle":"2025-09-15T17:54:19.755106Z","shell.execute_reply.started":"2025-09-15T17:54:08.482673Z","shell.execute_reply":"2025-09-15T17:54:19.754405Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-huggingface\n  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\nCollecting langchain-pinecone\n  Downloading langchain_pinecone-0.2.12-py3-none-any.whl.metadata (8.6 kB)\nCollecting langchain-core<1.0.0,>=0.3.70 (from langchain-huggingface)\n  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\nCollecting huggingface-hub>=0.33.4 (from langchain-huggingface)\n  Downloading huggingface_hub-0.34.5-py3-none-any.whl.metadata (14 kB)\nCollecting pinecone<8.0.0,>=6.0.0 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n  Downloading pinecone-7.3.0-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (1.26.4)\nCollecting langchain-openai>=0.3.11 (from langchain-pinecone)\n  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: httpx>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (0.28.1)\nRequirement already satisfied: simsimd>=5.9.11 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (6.4.9)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.0->langchain-pinecone) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.0->langchain-pinecone) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.0->langchain-pinecone) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.0->langchain-pinecone) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.0->langchain-pinecone) (0.16.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\nRequirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (8.5.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\nRequirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.7)\nCollecting openai<2.0.0,>=1.104.2 (from langchain-openai>=0.3.11->langchain-pinecone)\n  Downloading openai-1.107.2-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain-pinecone) (0.9.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->langchain-pinecone) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->langchain-pinecone) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->langchain-pinecone) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->langchain-pinecone) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->langchain-pinecone) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.4->langchain-pinecone) (2.4.1)\nCollecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n  Downloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl.metadata (30 kB)\nCollecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.5.0)\nRequirement already satisfied: aiohttp>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.12.13)\nCollecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.20.1)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.23.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.104.2->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\nCollecting packaging>=20.9 (from huggingface-hub>=0.33.4->langchain-huggingface)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.2)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain-pinecone) (2024.11.6)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->langchain-pinecone) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.4->langchain-pinecone) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.4->langchain-pinecone) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.4->langchain-pinecone) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.4->langchain-pinecone) (2024.2.0)\nDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\nDownloading langchain_pinecone-0.2.12-py3-none-any.whl (25 kB)\nDownloading huggingface_hub-0.34.5-py3-none-any.whl (562 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.2/562.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone-7.3.0-py3-none-any.whl (587 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\nDownloading openai-1.107.2-py3-none-any.whl (946 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m946.9/946.9 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_assistant-1.8.0-py3-none-any.whl (259 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\nInstalling collected packages: pinecone-plugin-interface, packaging, pinecone-plugin-assistant, huggingface-hub, pinecone, openai, aiohttp-retry, langchain-core, langchain-openai, langchain-huggingface, langchain-pinecone\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n  Attempting uninstall: openai\n    Found existing installation: openai 1.91.0\n    Uninstalling openai-1.91.0:\n      Successfully uninstalled openai-1.91.0\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.66\n    Uninstalling langchain-core-0.3.66:\n      Successfully uninstalled langchain-core-0.3.66\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohttp-retry-2.9.1 huggingface-hub-0.34.5 langchain-core-0.3.76 langchain-huggingface-0.3.1 langchain-openai-0.3.33 langchain-pinecone-0.2.12 openai-1.107.2 packaging-24.2 pinecone-7.3.0 pinecone-plugin-assistant-1.8.0 pinecone-plugin-interface-0.0.7\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nfrom typing import List\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_core.documents import Document\nfrom langchain_pinecone import PineconeVectorStore\nfrom pydantic import BaseModel\nfrom pinecone import Pinecone\nfrom uuid import uuid4\n\n# Config\nEMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\nSOURCE_DIRECTORY = \"/kaggle/input/dataset/DataSet\"\nPINECONE_ENVIRONMENT = \"us-east-1\"\n\npc = Pinecone(api_key=\"pcsk_3aziQH_FhYmdBpXgFTL6borDTmnUcMFSnsXd55CXq5YdYmge17qF14iQP7ZScvAXxnT261\")\nindex_name = \"lawyerlens-kb\"\nindex = pc.Index(index_name)\n\ndef load_json_files(json_paths: List[str]) -> List[Document]:\n    \"\"\"Load legal laws from JSON files and convert them into LangChain Documents.\"\"\"\n    documents = []\n    for path in json_paths:\n        try:\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n                # Handle case: dataset could be a list of laws\n                if isinstance(data, dict):\n                    data = [data]\n                for law in data:\n                    text = law.get(\"text\", \"\")\n                    notes = law.get(\"interpretation_notes\", \"\")\n                    metadata = {\n                        \"source_id\": law.get(\"source_id\", \"\"),\n                        \"jurisdiction\": law.get(\"jurisdiction\", \"\"),\n                        \"title\": law.get(\"title\", \"\"),\n                        \"section\": law.get(\"statute_section\", \"\"),\n                        \"topics\": law.get(\"topics\", []),\n                        \"keywords\": law.get(\"keywords\", []),\n                        \"risk_relevance\": json.dumps(law.get(\"risk_relevance\", {})),  # <-- FIX\n                        \"url\": law.get(\"source_url\", \"\")\n                    }\n                    content = f\"{text}\\n\\nNotes: {notes}\"\n                    documents.append(Document(page_content=content, metadata=metadata))\n        except Exception as e:\n            print(f\"Error reading {path}: {e}\")\n    return documents\n\ndef get_text_chunks(documents: List[Document]) -> List[Document]:\n    \"\"\"Split long text in documents into smaller chunks.\"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    split_docs = []\n    for doc in documents:\n        chunks = text_splitter.split_text(doc.page_content)\n        for chunk in chunks:\n            split_docs.append(Document(page_content=chunk, metadata=doc.metadata))\n    return split_docs\n\ndef main():\n    print(\"Building knowledge base and uploading to Pinecone...\")\n\n    # 1. Load source JSON files\n    json_paths = [os.path.join(SOURCE_DIRECTORY, f) for f in os.listdir(SOURCE_DIRECTORY) if f.endswith(\".json\")]\n    if not json_paths:\n        print(f\"No JSON documents found in the '{SOURCE_DIRECTORY}' folder.\")\n        return\n    print(f\"Found {len(json_paths)} JSON files to process.\")\n\n    # 2. Load JSON as documents\n    documents = load_json_files(json_paths)\n    print(f\"Loaded {len(documents)} laws from JSON.\")\n\n    # 3. Split into chunks\n    documents = get_text_chunks(documents)\n    print(f\"Split into {len(documents)} chunks.\")\n\n    # 4. Initialize the embedding model\n    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n    embeddings = HuggingFaceEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        model_kwargs={'device': 'cuda'}\n    )\n\n    # 5. Upload documents to Pinecone\n    print(f\"Uploading {len(documents)} chunks to Pinecone index '{index_name}'...\")\n    vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n    uuids = [str(uuid4()) for _ in range(len(documents))]\n    vector_store.add_documents(documents=documents, ids=uuids)\n\n    print(\"\\n ✅ Knowledge base uploaded to Pinecone successfully!\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T17:54:59.233485Z","iopub.execute_input":"2025-09-15T17:54:59.233791Z","iopub.status.idle":"2025-09-15T17:55:42.053822Z","shell.execute_reply.started":"2025-09-15T17:54:59.233769Z","shell.execute_reply":"2025-09-15T17:55:42.053038Z"}},"outputs":[{"name":"stdout","text":"Building knowledge base and uploading to Pinecone...\nFound 1 JSON files to process.\nLoaded 12 laws from JSON.\nSplit into 12 chunks.\nLoading embedding model: hkunlp/instructor-large\n","output_type":"stream"},{"name":"stderr","text":"2025-09-15 17:55:08.863837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757958909.174832      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757958909.263062      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8c7a5c647c54289b9d8dfd3ade2e6ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c4a15148ca340e893b66ba8af88032c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87c147b244494976b9db2f9f589fd2cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d07894b9609b4eb68e0c26bdfd1860f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0311da1c36c846a4aa36ac87b6e0d572"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe611c0295014ff2925209bc069411e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c549f81eb2e4925afe241638acac14e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f35af0f231ed40fdba890b7252478de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40fc2cc7d099448092238ba3b1461a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c00c03cbce649fb9a035964bf48cbb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2670c358612b477aafaeee226463faa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d39e51866a6b45c098fcf80050a82be6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd706dd0691479c9f41840112a6ea68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense/pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bfa80ff4eb34497a4e0024fa2d6dbab"}},"metadata":{}},{"name":"stdout","text":"Uploading 12 chunks to Pinecone index 'lawyerlens-kb'...\n\n ✅ Knowledge base uploaded to Pinecone successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport fitz  # PyMuPDF\nimport json\nimport streamlit as st\nfrom typing import List\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_pinecone import PineconeVectorStore\nfrom pinecone import Pinecone\nfrom langchain.chains import LLMChain\n\nEMBEDDING_MODEL_NAME = \"hkunlp/instructor-large\"\nLLM_MODEL_NAME = \"gemini-1.5-flash\"\nINDEX_NAME = \"lawyerlens-kb\"\nGOOGLE_API_KEY = \"AIzaSyDUkq0nrIXwPfVkS3tgagS7zV6EtCMPSss\"\nPINECONE_API_KEY = \"pcsk_3aziQH_FhYmdBpXgFTL6borDTmnUcMFSnsXd55CXq5YdYmge17qF14iQP7ZScvAXxnT261\"\n\n@st.cache_resource\ndef load_embeddings():\n    return HuggingFaceEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        model_kwargs={'device': 'cpu'}\n    )\n\n@st.cache_resource\ndef load_llm():\n    return ChatGoogleGenerativeAI(\n        model=LLM_MODEL_NAME,\n        google_api_key=GOOGLE_API_KEY,\n    )\n\n# Extract text from PDFs\ndef get_pdf_text(pdf_docs: List) -> str:\n    text = \"\"\n    for pdf_file in pdf_docs:\n        try:\n            with fitz.open(stream=pdf_file.read(), filetype=\"pdf\") as doc:\n                text += \"\".join(page.get_text() for page in doc)\n        except Exception as e:\n            st.error(f\"Error reading {pdf_file.name}: {e}\")\n    return text\n\ndef get_text_chunks(raw_text: str) -> List[str]:\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        length_function=len\n    )\n    return text_splitter.split_text(raw_text)\n\ndef main():\n    st.set_page_config(page_title=\"LawyerLens\", page_icon=\"⚖️\")\n    st.header(\"LawyerLens: AI Legal Assistant ⚖️\")\n\n    embeddings = load_embeddings()\n    llm = load_llm() \n    pc = Pinecone(api_key=PINECONE_API_KEY)\n    index = pc.Index(INDEX_NAME)\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n    if \"retriever_uploaded\" not in st.session_state:\n        st.session_state.retriever_uploaded = None\n\n    # Display chat history\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.markdown(message[\"content\"])\n\n    # Sidebar upload\n    with st.sidebar:\n        st.subheader(\"Upload Legal Documents\")\n        pdf_docs = st.file_uploader(\"Upload agreements (PDF)\", accept_multiple_files=True)\n        if st.button(\"Process\"):\n            if pdf_docs:\n                with st.spinner(\"Processing documents...\"):\n                    raw_text = get_pdf_text(pdf_docs)\n                    chunks = get_text_chunks(raw_text)\n                    vector_store_uploaded = FAISS.from_texts(texts=chunks, embedding=embeddings)\n                    st.session_state.retriever_uploaded = vector_store_uploaded.as_retriever()\n                    st.success(\"Uploaded documents processed ✅\")\n            else:\n                st.warning(\"Please upload at least one PDF file.\")\n\n    # Main chat\n    if user_prompt := st.chat_input(\"Ask a question about your document...\"):\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_prompt})\n        with st.chat_message(\"user\"):\n            st.markdown(user_prompt)\n\n        with st.chat_message(\"assistant\"):\n            if st.session_state.retriever_uploaded is None:\n                st.warning(\"Please upload and process a document first.\")\n                st.stop()\n\n            with st.spinner(\"Analyzing...\"):\n                # 1. Connect to Indian Law Knowledge Base\n                vector_store_law = PineconeVectorStore.from_existing_index(\n                    index_name=INDEX_NAME,\n                    embedding=embeddings\n                )\n                retriever_indian_law = vector_store_law.as_retriever(search_kwargs={'k': 5})\n\n                # 2. Retrieve from both sources\n                docs_from_law = retriever_indian_law.get_relevant_documents(user_prompt)\n                docs_from_upload = st.session_state.retriever_uploaded.get_relevant_documents(user_prompt)\n\n                context_law = \"\\n\".join([doc.page_content for doc in docs_from_law])\n                context_upload = \"\\n\".join([doc.page_content for doc in docs_from_upload])\n\n                # 3. Prompt template\n                template = \"\"\"\n                ROLE: You are an AI legal assistant with the expertise of a seasoned Indian High Court lawyer.\n\n                TASK: Compare the 'Uploaded Document Context' with the 'Indian Law Context' and provide a precise legal analysis.\n\n                - If there is a conflict → explain and state the correct law.\n                - If it aligns → confirm compliance.\n                - If law is silent → answer from uploaded document only.\n\n                ---\n                Indian Law Context:\n                {context_indian_law}\n\n                Uploaded Document Context:\n                {context_uploaded_doc}\n\n                User's Question:\n                {question}\n                ---\n                Legal Analysis:\n                \"\"\"\n\n                prompt_template = PromptTemplate(\n                    template=template,\n                    input_variables=[\"context_uploaded_doc\", \"context_indian_law\", \"question\"]\n                )\n\n                chain = LLMChain(llm=llm, prompt=prompt_template)\n                response = chain.invoke({\n                    \"context_uploaded_doc\": context_upload,\n                    \"context_indian_law\": context_law,\n                    \"question\": user_prompt\n                })\n\n                response_text = response['text']\n                st.markdown(response_text)\n                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response_text})\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}